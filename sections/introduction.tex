\section{Introduction}
\label{sec:introduction}

The variational auto-encoder is a successful class of hierarchical Bayesian models based on transforming data from a simple latent prior into a complicated observation distribution using a neural network.

\begin{align*}
    P(\x) &= \int_\x \Pdec \dz
\end{align*}

% Mutual information is a natural indicator of the quality of a learned representation
% \cite{hjelm2018learning}, along with other characteristics, such as the compositionality of
% latent factors, that are expected to be useful in downstream tasks, like transfer 
% learning \cite{DBLP:journals/corr/BengioTPPB17}.
% Mutual information is, however, computationally difficult to estimate for continuous
% high-dimensional random variables.  As such, it can be hard to optimize when learning
% latent variable models \cite{Hjelm2018,Chen2016}.

% % Many existing models are trained using approximate maximum likelihood,
% % the canonical example being the variational auto-encoder, or VAE \cite{Kingma2013,Rezende2014}.
% % The VAE comprises a probabilistic decoder and an approximate encoder, learned via
% % optimization of an evidence-based lower bound (ELBO) on the log marginal data distribution.
% % While often producing excellent representations, it is also well-known that the VAE
% % sometimes produces pathological results in which the encoder, or approximate
% % posterior, conveys relatively little useful information about the latent state.
% % This behavior, often referred to as {\em posterior collapse}, results in low
% % mutual information between observations and inferred latent states \cite{DBLP:journals/corr/BowmanVVDJB15,ChenKSDDSSA16,
% % DBLP:journals/corr/abs-1901-03416,DBLP:journals/corr/OordKK16,
% % DBLP:journals/corr/abs-1711-00937}.

% This paper formulates a new class of probabilistic auto-encoder model
% that is motivated by two key design principles, namely, the maximization
% of mutual information, and the symmetry of the encoder-decoder components.
% Symmetry captures our desire for both the encoder and decoder to effectively
% and consistently model the underlying observation and latent domains.
% This is particularly useful for downstream tasks in which either one
% or both of the encoder and decoder play a central role.
% These properties are formulated in terms of the symmetric Jensen-Shannon
% Divergence between the encoder and decoder, combined with an objective term
% to maximize mutual information.  We refer to the resulting model as the
% {\em mutual information machine}, or MIM.

% We contrast MIM with models trained using (approximate) maximum likelihood, 
% the canonical example being the variational auto-encoder, or VAE 
% \cite{Kingma2013,Rezende2014}. The VAE comprises a probabilistic decoder 
% and an approximate encoder, learned via optimization of an evidence-based 
% lower bound (ELBO) on the log marginal data distribution.  In contrast to
% MIM it is asymmetric in its formulation, and while often producing excellent 
% representations, VAEs sometimes produce pathological results in which the 
% encoder, or approximate posterior, conveys relatively little information about
% the latent state. This behavior, often referred to as {\em posterior collapse}, 
% results in low mutual information between observations and inferred latent 
% states \cite{DBLP:journals/corr/BowmanVVDJB15,ChenKSDDSSA16,
% DBLP:journals/corr/abs-1901-03416,DBLP:journals/corr/OordKK16,
% DBLP:journals/corr/abs-1711-00937}.

% In what follows we formulate the MIM model and propose a learning
% algorithm that minimizes an upper bound on the desired loss.
% We show that the resulting objective can be viewed as a symmetrized
% form of KL divergence, thereby closely related to the asymmetric KL
% objective of the VAE.
% In addition to the theoretical development we include several
% experiments that reveal properties of MIM through ablation studies,
% demonstrating the effectiveness of component terms in the objective.
% \micha{review me}
% This also enables direct comparisons to the VAE in terms of posterior
% collapse, mutual information, data log likelihood, and clustering.
% We demonstrate that MIM offers favourable mutual information, similar reconstruction, and better clustering in the latent representation, on the expense of sampling quality, when compared to VAE
% with the same architecture. We than demonstrate that given powerful enough model MIM can match the sampling quality of VAE.
