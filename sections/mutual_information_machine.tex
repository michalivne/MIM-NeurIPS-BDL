
\section{Mutual Information Machine}
\label{sec:mim}

Section \ref{sec:symmetry_and_mutual_information} formulates a loss function
\eqref{eq:jsd-h} that reflects our desire for model symmetry and high mutual information.
This objective is difficult to optimize directly since we do not know how to evaluate 
$\log\pjoint(\x)$ in the general case (\ie, we do not have an exact closed-form 
expression for $\pjoint(\x)$).
As a consequence, we introduce parameterized approximate priors, $\Menc(\x)$ and $\Mdec(\z)$,
to derive tractable bounds on the penalized Jensen-Shannon divergence.
This is similar in spirit to VAEs, which introduce a parameterized approximate posterior.
These parameterized priors, together with the conditional encoder and decoder,
$\Menc(\z | \x)$ and $\Mdec(\x | \z)$, comprise a new pair of joint distributions,
\begin{align*}
    \Menc (\x, \z ) & \equiv \Menc\left(\z| \x\right) \Menc\left(\x\right) \\
    \Mdec (\x, \z ) & \equiv \Mdec\left(\x| \z\right) \Mdec\left(\z\right)  ~.
\end{align*}

These new joint distributions allow us to formulate a new, tractable loss
that bounds $H(\Msamp)$:
% \begin{align*}
%     \CEloss(\params)~ &\equiv ~H(\Msamp, \Mmodel) \\
%     &= ~\DKL{\Msamp}{\Mmodel} + H(\Msamp) \\
%     &\geq ~H(\Msamp)
%     %% \MIMloss(\params) &\equiv \frac{1}{2} ( H(\Msamp, \Menc \left(\x, \z \right)) + H(\Msamp, \Mdec \left(\x, \z \right)))
% \end{align*}
\begin{eqnarray}
    \CEloss(\params)
    &\equiv& H(\Msamp, \Mmodel)   \nonumber \\
    &=&  \DKL{\Msamp}{\Mmodel} + H(\Msamp) \nonumber \\
    &\geq&  H(\Msamp) ~,
    \label{eq:LCE}
\end{eqnarray}
where $H(\Msamp, \Mmodel)$ denotes the cross-entropy between $\Msamp$ and $\Mmodel$,
and
\begin{equation}
    \Mmodel ~= ~ \frac{1}{2} \big( \, \Mdecjoint + \Mencjoint \, \big) ~.
\end{equation}
In what follows we refer to $\CEloss$ as the cross-entropy loss.
% In what follows we refer to $\CEloss$ as the cross entropy loss,
% and $\MIMloss$ as the mutual information machine (MIM) loss.
It aims to match the model prior distributions to the anchors, while also
minimizing $H(\Msamp)$. The main advantage of this formulation is that the
cross-entropy loss can be trained by Monte Carlo sampling from the anchor
distributions with the reparameterization trick~\citep{Kingma2013, Rezende2014}.

At this stage it might seem odd to introduce a parametric prior for $\pjoint(\z)$.
Indeed, setting it directly is certainly an option.
Nevertheless, in order to achieve consistency between $\Mdecjoint$ and $\Mencjoint$
it can be advantageous to allow $\Mdec(\z)$ to vary.
Essentially, we trade-off latent prior fidelity for increased model consistency.
We give more insight in the supplementary material. \kevin{double check this}

One issue with $\CEloss$ is that, while it will try to enforce consistency
between the model and the anchored distributions, i.e.,
$\Mdec (\x, \z ) \approx \Mdec(\x | \z)\pjoint(\z)$ and
$\Menc(\x, \z) \approx \Menc(\z | \x)\pjoint(\x)$, it will not directly
try to achieve model consistency: $\Mdec (\x, \z ) \approx \Menc(\x, \z)$.
To remedy this, we bound $\CEloss$ using Jensen's inequality, \ie,
\begin{align}
    \MIMloss(\params)~ &\equiv ~\frac{1}{2}
    \big(\, H(\Msamp, \Menc \left(\x, \z \right) ) + H(\Msamp, \Mdec \left(\x, \z \right)) \, \big)
    \label{eq:mimloss}  \\
    &\geq ~\,  \CEloss(\params) ~.
    \label{eq:mim_celoss}
\end{align}

Equation \eqref{eq:mimloss} gives us the loss function for the Mutual Information 
Machine (MIM).  It is an average of cross entropy terms between the mixture 
distribution $\Msamp$ and the model encoding and decoding distributions respectively.
To see that this encourages model consistency, it can be shown that $\MIMloss$ is
equivalent to $\CEloss$ plus a non-negative model consistency regularizer; i.e.,
\begin{align}
    \MIMloss(\params) ~= ~\CEloss(\params) + \RMIM(\params) ~.
    \label{eq:LMIM-LCE}
\end{align}
The non-negativity of $\RMIM$ is a simple consequence of 
$\MIMloss(\params) \ge \CEloss(\params)$ in \eqref{eq:mim_celoss}.
One can further show (see supplementary material) that $\RMIM(\params)$ satisfies
\begin{align}
    \RMIM(\params) ~& = ~\frac{1}{2}
    \big(\,\DKL{\Msamp}{\Mdecjoint} + \DKL{\Msamp}{\Mencjoint} \big) - \DKL{\Msamp}{\Mmodel} \label{eq:RMIM-DKL}  \\
    % &= ~ \MIMloss(\params) - \CEloss(\params) \nonumber \\
    &= ~ \E{\x,\z \sim \Msamp}{-\log \, \frac{\sqrt{\Menc \left(\x, \z \right) \cdot \Mdec \left(\x, \z \right)}}{\frac{1}{2} (\Menc \left(\x, \z \right) + \Mdec \left(\x, \z \right))} } 
    ~ .
    \label{eq:RMIM}
\end{align}
% The non-negativity of $\RMIM(\params)$ also follows from the
% fact that the geometric mean is always less than or equal to the algebraic 
% mean of two non-negative quantities. 
One can conclude from \eqref{eq:RMIM} that the regularizer $\RMIM$ is zero 
only when the two joint model distributions, $\Menc \left(\x, \z \right)$
and $\Mdec \left(\x, \z \right)$, are identical under fair samples from 
the joint sample distribution $\Msamp \left(\x, \z \right)$.
In practice we find that encouraging model consistency also helps stabilize learning.

To understand the MIM objective in greater depth, we find it helpful to express $\MIMloss$ 
as a sum of fundamental terms that provide some intuition for its expected behavior.  
In particular, as derived in the supplementary material:
\begin{align}
    \MIMloss(\params)~ &=  ~
    \mathrm{R}_\mathrm{H}(\params) \, +\,
    \frac{1}{4}\big(\, \DKL{\pjoint(\z)}{ \Mdec(\z)}  + \DKL{\pjoint(\x)}{\Menc(\x)} \big)
    \nonumber \\
    & \qquad
    +\, \frac{1}{4}\big(\,\DKL{\Penc }{ \Mdec(\x, \z)} + \DKL{\Pdec}{ \Menc(\z , \x)} \big)
    \label{eq:MIM-parts}
\end{align}
The first term in \eqref{eq:MIM-parts}, as discussed above, encourages high mutual
information between observations and latent states. The second term shows that MIM
directly encourages the model prior distributions to match the anchor distributions.
Indeed, the KL term between the data anchor and the model prior is the maximum
likelihood objective.
The third term encourages consistency between the model distributions and the
anchored distributions, in effect fitting the model decoder to samples drawn from
the anchored encoder (cf.\ VAE), and, via symmetry, fitting the model encoder to
samples drawn from the model decoder (both with reparameterization).
In this view, MIM can be seen as simultaneously training and distilling a model
distribution over the data into a latent variable model.
The idea of distilling density models has been used in other domains, e.g.,
for parallelizing auto-regressive models~\citep{oord2017parallel}.

% , however to our knowledge $\MIMloss$ is
% the first objective that can train both student and teacher simultaneously under a single principled loss function.
% Kevin: Don't want to oversell, because this would imply that we should compare to their approach.

In summary, the MIM loss can be viewed as an upper bound on the entropy
of a particular mixture distribution $\Msamp$:
\begin{eqnarray}
    \MIMloss(\params) &=& 
    ~\frac{1}{2} \big(\, H(\Msamp, \Menc \left(\x, \z \right) ) + 
    H(\Msamp, \Mdec \left(\x, \z \right)) \, \big) \nonumber \\
    &=& H(\Msamp, \Mmodel) + \RMIM(\params) \nonumber  \\
    &\geq& H(\Msamp, \Mmodel) \nonumber \\
    &\geq& H(\Msamp) \nonumber  \\
    &=&  H_{\Msamp} (\x) + H_{\Msamp} (\z) - I_{\Msamp} (\x;\z) 
    \label{eq:MIM-symmetric-bound}
\end{eqnarray}
Through the MIM loss and the introduction of the parameterized model
distribution $\Mmodel$, we are pushing down on the entropy of the anchored
mixture distribution $\Msamp$, which is the sum of marginal entropies minus 
the mutual information.  Minimizing the MIM bound yields consistency of the 
model encoder and decoder, and high mutual information of $\Msamp$ between 
observations and latent states.

% The relations in Equation \eqref{eq:MIM-symmetric-bound} also offer some insight
% into the tightness of the bounds.
% The effects of minimizing $\MIMloss(\params)$ are twofold. The minimization of the
% consistency regularizer $\RMIM(\params)$ in the upper bound also directly affects
% the target objective $H(\Msamp)$, by virtue of sharing encoder and decoder parameters.
% Effectively, by shrinking the consistency gap $\RMIM(\params)$, either $\MIMloss$ minimizes the upper bound on the cross entropy $H(\Msamp, \Mmodel)$, or the entropy $H(\Msamp)$ increases the lower bound of the cross entropy.
% In other words, MIM learning is effectively changing the optimization problem (\ie, by altering $H(\Msamp)$)to be easier for the model to learn, while also changing the model to better fit the data.
% This "dual" optimization can be understood as a symmetric equivalent of the
% optimization of a single bound in VAE learning.
% \david{Still not sure I understand this fully.}
% \micha{I clarified it}
% \kevin{This might be worth talking about then. It could be helpful to consider (for our own understanding, not the paper) H(P, Q) for any P and Q where we are allowed to vary one, the other, or both.}

% The \MIM learning objective relates to MI as follows,
% \begin{align}
% I_{\Msamp} (\x;\z) -H_{\Msamp(\x)} ( \x ) -H_{\Msamp(\z)} ( \z ) ~ &  = -H_{\Msamp}( \x, \z ) \\
% & \ge  -H( \Msamp, \Mmodel ) \\
% &  \ge -\frac{1}{2} \left(H \left( \Msamp, \Menc \right)+H \left( \Msamp, \Mdec \right)\right)  \label{eq:mim-mi-lower-bound} \\
%  & = -H \left( \Msamp, \Mmodel \right) -\RMIM(\params)
% \end{align}
% where Equation \eqref{eq:mim-mi-lower-bound} is the objective in a maximization optimization problem. The above relation offers insights regarding the tightness of the lower bound. Maximizing Equation \eqref{eq:mim-mi-lower-bound} pushes up the lower bound, while simultaneously pulling down the upper bound (\ie, the upper bound shares the parametric encoder and decoder). This relation can be viewed as a student-teacher joint optimization. The lower bound improves the model $\Mmodel$ (\ie student), while the upper bound adapts the optimization problem (\ie, the teacher) in order to close the gap. This optimization process occurs concurrently with pushing both bound (lower and upper) up, as the cross entropy term $H( \Msamp, \Mmodel )$ appears in both.


% \begin{align}
% R_{\params}\left(\x,\z \right) & = \log \left(\frac{\sqrt{\Menc(\x,\z) \cdot \Mdec(\x,\z)}}{\frac{1}{2} \left( \Menc(\x,\z)+\Mdec(\x,\z) \right)}  \right) \le 0 \label{eq:mim-consistency-regularizer} \\
% \RMIM(\params)  & = -\E{\Msamp}{R_{\params}} \ge 0
% \end{align}


% \kevin{either put the figure here or refer to supplementary material.}
% \micha{I believe it is important to have the explicit relation to MI due to lower bound over entropy (either here or in the previous section). In my understanding MIM works well because it is an efficient/tractable lower bound for maximizing the MI of $\Msamp$. $\Mmodel$ is an auxiliary tool to train for encoder/decoder to do so + supporting priors to make it tractable. In the current formulation it is hidden.}

% \begin{equation}
% R_{\params}\left(\x,\z \right) = \log \left(\frac{\sqrt{\Menc(\x,\z) \cdot \Mdec(\x,\z)}}{\frac{1}{2} \left( \Menc(\x,\z)+\Mdec(\x,\z) \right)}  \right) \le 0 \label{eq:mim-consistency-regularizer}
% \end{equation}
% \begin{align}
% I_{\Msamp} (\x;\z) -H_{\Msamp(\x)} ( \x ) -H_{\Msamp(\z)} ( \z ) ~ &  = -H_{\Msamp}( \x, \z ) \\
% & \ge  -H( \Msamp, \Mmodel ) \\
% &  \ge -\frac{1}{2} \left(H \left( \Msamp, \Menc \right)+H \left( \Msamp, \Mdec \right)\right)  \\
%  & = -H \left( \Msamp, \Mmodel \right) + \E{\Msamp}{R_{\params}}
%  \label{eq:mim-mi-bound}
% \end{align}

% \micha{The stroy in my mind.
% [CONCEPTUAL]: \MIM is a tractable learning method to maximize MI of a joint distribution with anchors ($\Msamp$) and corresponding encoder and decoder. Since it hard (anchors might not have analytic form) we introduce $\Mmodel$ with auxiliary priors. The priors are there to make the learning tractable, but it turns out they are also useful by themselves.
% [INTERPRETABLE]: Breaking lower bound to interpertable expressions, and justifying CE as a loss. This allows us to design experiments, and demonstrate the mechanics of \MIM. }
