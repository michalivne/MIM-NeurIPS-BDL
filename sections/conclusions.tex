
\section{Related Work}
\label{sec:related-work}

The VAE \cite{Kingma2013} is a latent variable model (LVM) that is widely used in learning a useful latent representation.
Typically, the quality of the learned representation is measured by auxiliary tasks such as classification \cite{DBLP:journals/corr/BengioTPPB17}.
The VAE also provides a remarkable sampling capability (\eg, \cite{DBLP:journals/corr/abs-1901-03416}) which is considered as an evidence for 
the quality of the learned representation. Unfortunately, it has been observed that a powerful decoder
can suffer from posterior collapse \citep{DBLP:journals/corr/BowmanVVDJB15,ChenKSDDSSA16,
DBLP:journals/corr/abs-1901-03416,DBLP:journals/corr/OordKK16,
DBLP:journals/corr/abs-1711-00937}. where such a decoder ignores the encoder in some dimensions, 
and the learned representation has low mutual information with the observations.
While several methods has been proposed to mitigate the problem \cite{DBLP:journals/corr/abs-1711-00464, DBLP:journals/corr/abs-1901-03416}, no root cause has been suggested to date.

In addition, mutual information has been used to measure the quality of the representation \cite{Hjelm2018,hjelm2018learning}. Mutual information, together with disentanglement, are considered as corner stones 
of a useful representation. Normalizing flows \cite{Rezende2015,Dinh2014,Dinh2016a,Kingma2018,DBLP:journals/corr/abs-1902-00275} directly maximizes mutual information
by restricting the architecture to be invertible and tractable. This, however, requires the latent dimension to be the same as the dimension of the observations (\ie, no bottleneck).
As a result no information is lost, and thus normalizing flows cannot be used in order to learn a concise representation of high dimensional data (\eg, images). Here, MIM can be viewed as a generalization of invertibility, which supports change of dimensionality, and behaves as an invertible model when dimensionality is the same.

A closer formulation to \MIM is proposed in \cite{DBLP:journals/corr/BornscheinSFB15}, which shares several of the design
principles with our work, including symmetry (\ie, consistency of encoding and decoding distributions). 
One importance difference however, is that the formulation proposed in \cite{DBLP:journals/corr/BornscheinSFB15} advocates for a joint density
in terms of the geometric mean between the encoder and decoder, for which they have to 
compute the partition function.  As a result the model learning is computationally very expensive.

Alternatively,  GANs \cite{NIPS2014_5423}, which for the most part focus on decoder properties without a proper inference model,
has been shown to minimize JSD between the observations anchor $\pjoint(\x)$ and the model generative process $\Menc(\x)$ (\ie, the marginal of the decoding distribution in \MIM terms). 
In particular, \citep{DonahueKD16-BiGAN,Bang-BiGAN2018} recognizes the importance of symmetry in learning generative models with reference to symmetric discriminators on $x$ and $z$. In contrast, here we target JSD between the joint encoding and decoding distributions, together with a regularizer to encourage high mutual information within a consistent model.

<<<<<<< HEAD
% Related by different techniques:
% \begin{itemize}
%     \item GANs, for the most part focus on decoder properties without a proper inference model.
%     There is however related work in recognizing the importance of symmetry in learning generative
%     models \citep{DonahueKD16-BiGAN,Bang-BiGAN2018} with reference to symmetric discriminators on $x$ and $z$.  
%     (Talk about GANs as optimizing JSD?)
    
%     \item related to normalizing flows (eg \citep{Dinh2016a}), which maximize mutual information,
%     one of the key goals of this work, but they have relatively limited archiecture for which 
%     the mapping is invertaible so the latent dimension is the same as the dimension of the observations.
    
%     \item there is the bidirectional Helmholtz Machine \citep{DBLP:journals/corr/BornscheinSFB15}, 
%     which shares several of the design
%     principles with our work, including symmetry, but the formulation advoates a joint density
%     in terms of the geometric mean between the encoder and decoder for which they have to 
%     compute the partition function.  Thus model learning is computationally very expensive.
% \end{itemize}

% It is possible to relate the JSD to the symmetric KL divergence through 
% the following bound, with the proof given in the supplementary material;
% i.e.,
% \begin{align*}
%     % \mathrm{JSD}(\params)~ &\leq ~
%     % \frac{1}{4} \Big ( \,\DKL{\Pdec}{\Penc} \\
%     % &\qquad+\DKL{\Penc}{\Pdec}\, \Big ) \\
%     \mathrm{JSD}(\params)~ &\leq ~
%     \frac{1}{4} \Big ( \,\DKL{\Pdec}{\Penc} + \DKL{\Penc}{\Pdec}\, \Big ) \\
%     ~ &\equiv	~ \mathrm{SKL}(\params)
% \end{align*}
% Notice that the second term of $\mathrm{SKL}$ is the VAE loss in \eqref{eq:vae-kl}.

% The same regularizer links the symmetric KL divergence to cross entropy as follows,
% \begin{equation}
%     \mathrm{SKL}(\params) + \mathrm{R}_\mathrm{H}(\params) ~=~ 
%     \frac{1}{2} \big(\, H(\Msamp, \Pdec) + H(\Msamp, \Penc) \,\big) ~,
%     \label{eq:skl-ce}
% \end{equation}
% where $H(\cdot, \cdot )$ denotes the usual cross-entropy.

% Even though $\MIMloss(\params)$ is a cross entropy loss that looks very similar to \eqref{eq:skl-ce}, it is not in general an upper bound on symmetric KL divergence. 
% Nevertheless, when $\Menc(\x)=\pjoint(\x)$ and $\Mdec(\z)=\pjoint(\z)$, i.e., when the model priors 
% equal the anchors, then $\CEloss(\params) = \mathrm{JSD}(\params) + \mathrm{R}_\mathrm{H}(\params)$ 
% and $\MIMloss(\params) = \mathrm{SKL}(\params) + \mathrm{R}_\mathrm{H}(\params)$.

=======
>>>>>>> overleaf-2019-09-09-1959

\section{Discussion and Conclusions} \label{sec:conclusion}


We introduce a new representation learning framework, named the {\em mutual information machine} (MIM), that defines a generative model which directly targets high mutual information (\ie, between the observations and the latent representation), and symmetry (\ie, consistency of encoding and decoding factorizations of the joint distribution). We derive a variational upper bound that enables the maximizion of mutual information in the learned representation for high dimensional continuous data, without the need to directly compute it. We then provide a possible explanation for the phenomena of posterior collapse, demonstrate  its validity, and show that MIM does not suffer from it. We also provide a comparison to VAE, and demonstrate that MIM learning leads to higher mutual information, and better clustering in the latent representation, for the same parametrization of the model. 

In addition, we show that MIM behaves similar to a deterministic autoencoder when the dimensionality of the latent representation is equal to that of the observations, which effectively provides invertibility via deterministic encoder-decoder. Such behaviour allows for training of invertible functions without any constraint on the architecture. Furthermore, it allows to generalize invertibility when the dimensionality differs with a probabilistic invertibility that is defined through consistency and high mutual information.

Two open questions that were not addressed in this paper: how to utilize a high capacity generative model with clustered latent representation? What is the significance of symmetry (multiple factorizations of the same distribution)? We leave that discussion for future work, where we will introduce the concept of "knowledge" - binary partition of the latent representation around areas of interest in the latent representation.
<<<<<<< HEAD

% Properties of the approach worth noting:
% \begin{itemize}
%     \item Mention we present a high capacity model (high MI), and that the utilization of the learned representation will be explored in follow up research.
%     \item We present a method to maximise mutual information without the need to evaluate it directly.
%     \item We provide a definition for posterior collapse, demonstrate  its validity, and introduce a new representation learning framework which does not suffer from it.
%     \item freedom to choose complex latent priors (cf. methods for VAE learning with GMMs for $p(z)$.
%     \item posterior collapse not seen, purportedly because MI remains high and encoder and decoder are similarly powerful.
%     \item single graphic model even when encoder and decoder not identical (ie $ \mathcal{M}_{\theta}$).
%     In that case encoder and decoder are approximate models for inference and sampling and density evaluation.
%     \item relation between use of JSD and symmetric KL as different possible symmetric divergence measures
%       (SEE supplementary material)
%     \item Two open questions that were not addressed in this paper: how to utilize a high capacity generative model with clustered latent representation? What is the significance of symmetry (multiple factorizations of the same distribution)? We leave that discussion for future work, where we will introduce the concept of "knowledge" - multiple binary partitions of the latent representation around areas of interest in $\z$.
%     \item Compared to a flow, MIM provides invertibility via deterministic encoder-decoder, but generalizes it by supporting dimensionality reduction.
% \end{itemize}
=======
>>>>>>> overleaf-2019-09-09-1959
