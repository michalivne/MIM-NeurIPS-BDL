\begin{abstract}
% We introduce the Mutual Information Machine (\MIM), a novel formulation of representation learning by cross entropy minimization 
% of the joint distribution over the observations and latent state. This is in contrast to the common
% use of data marginal entropy minimization in VAE.
% The proposed model, offers a new learning method which resembles a symmetric VI, but with
% a measurable lower bound gap.
% We start by showing that the observed phenomenon of posterior collapse in VAE is an optimal solution 
% when viewing VI as a cross entropy minimization problem over the joint distribution.
% We then derive a new learning framework to train an encoder-decoder of a joint distribution that maximizes
% the MI between the observations and the latent state. The  proposed 
% model offers superior reconstruction, higher MI, and as a result completely avoids posterior collapse,
% when compared to VAE.
% To the best of our knowledge, this paper presents a first explanation
% to posterior collapse in representation learning with VAE.
    We introduce the Mutual Information Machine (MIM), a novel formulation of representation learning, using a joint distribution over the observations and latent state in an encoder/decoder framework. Our key principles are symmetry and mutual information, where symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, and mutual information, to encourage the learning of useful representations for downstream tasks. Our starting point is the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, plus a mutual information encouraging regularizer. We show that this can be bounded by a tractable cross entropy loss function between the true model and a parameterized approximation, and relate this to the maximum likelihood framework. We also relate MIM to variational autoencoders (VAEs) and demonstrate that MIM is capable of learning symmetric factorizations, with high mutual information that avoids posterior collapse.
\end{abstract}
