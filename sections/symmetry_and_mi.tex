
\section{Symmetry and Mutual Information}
\label{sec:symmetry_and_mutual_information}

\begin{figure}
% \begin{wrapfigure}{R}{0.29\textwidth}
    \centering
    \input{diag/encoder-decoder}
    \caption{A \MIM model learns two factorizations of a joint distribution.
    (a) Encoding factorization. (b) Decoding factorization.
    (c) The estimated joint distribution.
    % (undirected graphical model).
    }
    \label{fig:mim-model}
% \end{wrapfigure}
\end{figure}

Our goal is to find a consistent encoder-decoder pair, representing a joint
distribution over the observation and latent domains, with high mutual
information between observations and latent states. By consistent, we mean 
that the encoding and decoding distributions $\Menc(\z | \x)\, \pjoint(\x)$ 
and $\Mdec(\x | \z)\, \pjoint(\z)$, define the same joint distribution.
Figure \ref{fig:mim-model} depicts this basic idea, in which the distribution is
valid and identical under both the encoding and decoding distributions, and is, in a
sense, an undirected model with two valid factorizations.  We note that consistency 
is achievable in the VAE when the approximate posterior $\Menc(\z | \x)$ is capable 
of representing the posterior under the decoding distribution $\Mdec(\z | \x)$.
In the general case, however, consistency is not usually achieved.

In contrast to the asymmetric divergence between encoding and decoding
distributions in the VAE objective \eqref{eq:vae-kl}, here we consider a
symmetric measure, namely, the well-known Jensen-Shannon divergence (JSD),
\begin{equation}
    \mathrm{JSD}(\params) ~=~
    \frac{1}{2}\Big( \, \DKL{\Pdec}{\Msamp} + \DKL{\Penc}{\Msamp}\, \Big) ~,
\end{equation}
where $\Msamp$ is an equally weighted mixture of the encoding
and decoding distributions; i.e.,
\begin{equation}
   \Msamp ~=~ \frac{1}{2} \big( \,\Pdec + \Penc \, \big)  ~.
   \label{eqn:msamp}
\end{equation}

Symmetry emphasizes consistency between the encoding and decoding distributions.
To learn useful latent representations we also want high mutual information 
between $x$ and $z$.
Indeed, the link between mutual information and representation learning has been explored
in many works in the literature \citep{hjelm2018learning,Hjelm2018,Chen2016}.
Here, to emphasize high mutual information, we add a particular regularizer of the form
\begin{align}
    \mathrm{R}_\mathrm{H}(\params) &= ~\frac{1}{2} \big( \, H(\Pdec) + H(\Penc) \, \big) 
    ~ .
    \label{eq:mi-regularizer}
\end{align}
This is the average of the joint entropy over $\x$ and $\z$ according
to the encoding and decoding distributions. This is related to mutual
information by the identity $H(\x, \z) = H(\x) + H(\z) - I(\x; \z)$.
That is, minimizing joint entropy encourages the minimization of the marginal entropy
and maximization of the mutual information. 
In addition to encouraging high mutual information, one can show that this
particular regularizer has a deep connection to JSD and entropy of $\Msamp$, i.e., 
\begin{equation}
    \mathrm{JSD}(\params) + \mathrm{R}_\mathrm{H}(\params) ~=~ H(\Msamp) ~.
    \label{eq:jsd-h}
\end{equation}
The derivation for Equation \eqref{eq:jsd-h} is
given in the supplementary material.
